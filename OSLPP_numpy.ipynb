{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.linalg\n",
    "from dataclasses import dataclass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def _load_tensors(domain):\n",
    "    mapping = {\n",
    "        'art': 'Art',\n",
    "        'clipart': 'Clipart',\n",
    "        'product': 'Product',\n",
    "        'real_world': 'RealWorld'\n",
    "    }\n",
    "    mat = scipy.io.loadmat(f'mats/OfficeHome-{mapping[domain]}-resnet50-noft.mat')\n",
    "    features, labels = mat['resnet50_features'], mat['labels']\n",
    "    features, labels = features[:,:,0,0], labels[0]\n",
    "    assert len(features) == len(labels)\n",
    "    # features, labels = torch.tensor(features), torch.tensor(labels)\n",
    "    # features = torch.load(f'./data_handling/features/OH_{domain}_features.pt')\n",
    "    # labels = torch.load(f'./data_handling/features/OH_{domain}_labels.pt')\n",
    "    return features, labels\n",
    "\n",
    "def create_datasets(source, target, num_src_classes, num_total_classes):\n",
    "    src_features, src_labels = _load_tensors(source)\n",
    "    idxs = src_labels < num_src_classes\n",
    "    src_features, src_labels = src_features[idxs], src_labels[idxs]\n",
    "\n",
    "    tgt_features, tgt_labels = _load_tensors(target)\n",
    "    idxs = tgt_labels < num_total_classes\n",
    "    tgt_features, tgt_labels = tgt_features[idxs], tgt_labels[idxs]\n",
    "    tgt_labels[tgt_labels >= num_src_classes] = num_src_classes\n",
    "\n",
    "    return (src_features, src_labels), (tgt_features, tgt_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_l2_norm(features:np.ndarray): return np.sqrt(np.square(features).sum(axis=1)).reshape((-1,1))\n",
    "def get_l2_normalized(features:np.ndarray): return features / get_l2_norm(features)\n",
    "def get_PCA(features, dim):\n",
    "    result = PCA(n_components=dim).fit_transform(features)\n",
    "    assert len(features) == len(result)\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_W(labels): return (labels.reshape(-1,1) == labels).astype(np.int)\n",
    "def get_D(W): return np.eye(len(W), dtype=np.int) * W.sum(axis=1)\n",
    "def fix_numerical_assymetry(M): return (M + M.transpose()) * 0.5\n",
    "def get_projection_matrix(features, labels, proj_dim):\n",
    "    N, d = features.shape\n",
    "    X = features.transpose()\n",
    "    \n",
    "    W = get_W(labels)\n",
    "    D = get_D(W)\n",
    "    L = D - W\n",
    "\n",
    "    A = fix_numerical_assymetry(np.matmul(np.matmul(X, D), X.transpose()))\n",
    "    B = fix_numerical_assymetry(np.matmul(np.matmul(X, L), X.transpose()) + np.eye(d))\n",
    "    assert (A.transpose() == A).all() and (B.transpose() == B).all()\n",
    "\n",
    "    w, v = scipy.linalg.eigh(A, B)\n",
    "    assert w[0] < w[-1]\n",
    "    w, v = w[-proj_dim:], v[:, -proj_dim:]\n",
    "    assert np.abs(np.matmul(A, v) - w * np.matmul(B, v)).max() < 1e-8\n",
    "    return v\n",
    "def project_features(P, features):\n",
    "    # P: pca_dim x proj_dim\n",
    "    # features: N x pca_dim\n",
    "    # result: N x proj_dim\n",
    "    return np.matmul(P.transpose(), features.transpose()).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_centroids(features, labels): \n",
    "    return np.stack([features[labels == c].mean(axis=0) for c in np.unique(labels)], axis=0)\n",
    "def get_dist(f, features):\n",
    "    return get_l2_norm(f - features)\n",
    "def get_closed_set_pseudo_labels(features_S, labels_S, features_T):\n",
    "    centroids = get_centroids(features_S, labels_S)\n",
    "    dists = np.stack([get_dist(f, centroids)[:,0] for f in features_T], axis=0)\n",
    "    pseudo_labels = np.argmin(dists, axis=1)\n",
    "    pseudo_probs = np.exp(-dists[np.arange(len(dists)), pseudo_labels]) / np.exp(-dists).sum(axis=1)\n",
    "    return pseudo_labels, pseudo_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def select_initial_rejected(pseudo_probs, n_r):\n",
    "    is_rejected = np.zeros((len(pseudo_probs),), dtype=np.int)\n",
    "    is_rejected[np.argsort(pseudo_probs)[:n_r]] = 1\n",
    "    return is_rejected"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def select_closed_set_pseudo_labels(pseudo_labels, pseudo_probs, is_rejected, t, T):\n",
    "    selected = np.ones_like(is_rejected) * -1\n",
    "    for c in np.unique(pseudo_labels):\n",
    "        Nc = (pseudo_labels == c).sum()\n",
    "        idxs = np.where((pseudo_labels == c) * (is_rejected == 0))[0]\n",
    "        idxs2 = idxs[np.argsort(pseudo_probs[idxs])[-((t+1)*Nc//T):]]\n",
    "        assert (selected[idxs2] == -1).all()\n",
    "        selected[idxs2] = c\n",
    "    assert (selected[is_rejected == 1] == -1).all()\n",
    "    return selected    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def update_rejected(selected_cs_pseudo_labels, is_rejected, features_T):\n",
    "    unlabeled = (selected_cs_pseudo_labels == -1) * (is_rejected == 0)\n",
    "    labeled_idxs = np.where(~unlabeled)[0]\n",
    "    new_is_rejected = is_rejected.copy()\n",
    "    for idx in np.where(unlabeled)[0]:\n",
    "        dists = get_dist(features_T[idx], features_T[labeled_idxs])\n",
    "        nn_idx = dists.argmin()\n",
    "        if is_rejected[labeled_idxs[nn_idx]]:\n",
    "            new_is_rejected[idx] = 1\n",
    "    assert (new_is_rejected[is_rejected == 1] == 1).all()\n",
    "    assert (new_is_rejected[selected_cs_pseudo_labels >= 0] == 0).all()\n",
    "    return new_is_rejected"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def evaluate(predicted, labels, num_src_classes):\n",
    "    acc_unk = (predicted[labels == num_src_classes] == labels[labels == num_src_classes]).mean()\n",
    "    accs = [(predicted[labels == c] == labels[labels == c]).mean() for c in range(num_src_classes)]\n",
    "    acc_common = np.array(accs).mean()\n",
    "    hos = 2 * acc_unk * acc_common / (acc_unk + acc_common)\n",
    "    _os = np.array(accs+[acc_unk]).mean()\n",
    "    return f'OS={_os*100:.2f} OS*={acc_common*100:.2f} unk={acc_unk*100:.2f} HOS={hos*100:.2f}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "@dataclass\n",
    "class Params:\n",
    "    pca_dim: int # = 512\n",
    "    proj_dim: int # = 128\n",
    "    T: int # = 10\n",
    "    n_r: int #  = 1200\n",
    "    dataset: str # = 'OfficeHome'\n",
    "    source: str # = 'art'\n",
    "    target: str # = 'clipart'\n",
    "    num_src_classes: int # = 25\n",
    "    num_total_classes: int # = 65\n",
    "    l2_first: bool # = True\n",
    "    center_and_normalzie: bool # = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def create_params(l2_first, center_and_normalzie):\n",
    "    return Params(pca_dim=512, proj_dim=128, T=10, n_r=1200, \n",
    "                  dataset='OfficeHome', source='art', target='clipart',\n",
    "                  num_src_classes=25, num_total_classes=65,\n",
    "                  l2_first=l2_first, center_and_normalzie=center_and_normalzie)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def _do_l2_normalization(feats_S, feats_T):\n",
    "    feats_S, feats_T = get_l2_normalized(feats_S), get_l2_normalized(feats_T)\n",
    "    assert np.abs(get_l2_norm(feats_S) - 1.).max() < 1e-6\n",
    "    assert np.abs(get_l2_norm(feats_T) - 1.).max() < 1e-6\n",
    "    return feats_S, feats_T\n",
    "\n",
    "def _do_pca(feats_S, feats_T):\n",
    "    feats_S, feats_T = get_PCA(feats_S, params.pca_dim), get_PCA(feats_T, params.pca_dim)\n",
    "    print('data shapes: ', feats_S.shape, feats_T.shape)\n",
    "    return feats_S, feats_T\n",
    "\n",
    "def _center_and_l2_normalize(zs_S, zs_T):\n",
    "    # center\n",
    "    zs_mean = np.concatenate((zs_S, zs_T), axis=0).mean(axis=0).reshape((1,-1))\n",
    "    zs_S = zs_S - zs_mean\n",
    "    zs_T = zs_T - zs_mean\n",
    "    # l2 normalize\n",
    "    zs_S, zs_T = _do_l2_normalization(zs_S, zs_T)\n",
    "    return zs_S, zs_T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def main(params:Params):\n",
    "    (feats_S, lbls_S), (feats_T, lbls_T) = create_datasets(params.source, params.target, params.num_src_classes, params.num_total_classes)\n",
    "    assert (np.unique(lbls_S) == np.arange(0, params.num_src_classes)).all()\n",
    "    assert (np.unique(lbls_T) == np.arange(0, params.num_src_classes+1)).all()\n",
    "    assert len(feats_S) == len(lbls_S)\n",
    "    assert len(feats_T) == len(lbls_T)\n",
    "    len(lbls_S), len(lbls_T)\n",
    "\n",
    "    # l2 normalization and pca\n",
    "    if params.l2_first:\n",
    "        feats_S, feats_T = _do_l2_normalization(feats_S, feats_T)\n",
    "        feats_S, feats_T = _do_pca(feats_S, feats_T)\n",
    "    else:\n",
    "        feats_S, feats_T = _do_l2_normalization(feats_S, feats_T)\n",
    "        feats_S, feats_T = _do_pca(feats_S, feats_T)\n",
    "\n",
    "    # initial\n",
    "    P = get_projection_matrix(feats_S, lbls_S, params.proj_dim)\n",
    "    zs_S, zs_T = project_features(P, feats_S), project_features(P, feats_T)\n",
    "    if params.center_and_normalzie: zs_S, zs_T = _center_and_l2_normalize(zs_S, zs_T)\n",
    "    pseudo_labels, pseudo_probs = get_closed_set_pseudo_labels(zs_S, lbls_S, zs_T)\n",
    "    is_rejected = select_initial_rejected(pseudo_probs, params.n_r)\n",
    "\n",
    "    # iterations\n",
    "    for t in range(1, params.T+1):\n",
    "        selected = select_closed_set_pseudo_labels(pseudo_labels, pseudo_probs, is_rejected, t, params.T)\n",
    "        is_rejected = update_rejected(selected, is_rejected, zs_T)\n",
    "        selected[is_rejected == 1] = params.num_src_classes\n",
    "        P = get_projection_matrix(np.concatenate((feats_S, feats_T[selected >= 0]), axis=0), \n",
    "                                np.concatenate((lbls_S, lbls_T[selected >= 0]), axis=0), \n",
    "                                params.proj_dim)\n",
    "        zs_S, zs_T = project_features(P, feats_S), project_features(P, feats_T)\n",
    "        if params.center_and_normalzie: zs_S, zs_T = _center_and_l2_normalize(zs_S, zs_T)\n",
    "        pseudo_labels, pseudo_probs = get_closed_set_pseudo_labels(zs_S, lbls_S, zs_T)\n",
    "\n",
    "    # final pseudo labels\n",
    "    pseudo_labels[is_rejected == 1] = params.num_src_classes\n",
    "\n",
    "    # evaluation\n",
    "    print(evaluate(pseudo_labels, lbls_T, params.num_src_classes))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "params = create_params(l2_first=True, center_and_normalzie=False)\n",
    "main(params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data shapes:  (1089, 512) (4365, 512)\n",
      "OS=6.35 OS*=4.26 unk=58.55 HOS=7.95\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "params = create_params(l2_first=False, center_and_normalzie=False)\n",
    "main(params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<__main__.test object at 0x7f804efcb5f8>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "params = create_params(l2_first=True, center_and_normalzie=True)\n",
    "main(params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "params = create_params(l2_first=False, center_and_normalzie=True)\n",
    "main(params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('main36': conda)"
  },
  "interpreter": {
   "hash": "87c3e150e0f22e62286b0675a541f4baa4e53a56f1434145374688b05f7921a7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}